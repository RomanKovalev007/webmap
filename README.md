# webmap

Данный краулер служит для парсинга веб сайтов и поиска на его страницах ссылок на другие веб страницы

Краулер реализован с помощью инструментов для реализации конкурентности на языке Go, а именно - горутин и каналов
    
    Программа принимает на вход:
    - веб страницу, с которой начинается парсинг
    - максимальное количество страниц, которые будут обработаны
    - максимальная глубина страницы относительно начальной
    - максимальное время работы программы, после которого программа будет приостоновленна
    - количество работающих воркеров-обработчиков(горутин)

    Работает программа следующим образом:
    На вход подаются начальные данные, описанные сверху,
    создаются два канала - канал задач с буффером и результирующий канал,
    затем начальная задача отправляется в канал задач,
    с помощью воркерпула запускаются горутины-обработчики, которые:
        1) читают из канала задач
        2) с помощью реализованной функции парсинга веб страниц находятся все ссылки на другие веб страницы на текущей странице
        3) с помощью цикла обрабатывают список найденных веб страниц и отправляют их в канал результатов.
    Далее в мэйне идет чтение из канала результатов, ссылки проверяются и подходящие отправляются обратно в канал задач.
    Итак по кругу.
    Программа завершается:
    - при достижении лимита найденных страниц
    - при достижении максимального времени работы программы (реализованно с помощью контекста)
    - если в канал задач перестают приходить задачи(все страницы обработаны)
    В конце работы программы выводятся ее результы.



